\documentclass[10pt]{article}\usepackage{epsfig}\newcommand{\done}{\rule{2mm}{2mm} \vskip \belowdisplayskip}\tolerance=10000\setlength{\oddsidemargin}{0in}\setlength{\topmargin}{0in}\setlength{\leftmargin}{0in}\setlength{\textwidth}{7.0in}\setlength{\textheight}{9.8in}\hoffset=-1truecm\voffset=-1.9truecm\raggedbottom\def\e {\`e }\def\a {\`a }\def\ii {\`\i }\def\o {\`o }\def\u {\`u }\def\E {\`E }\def\A {\`A }\def\II {\`\I }\def\O {\`O }\def\U {\`U }\begin{document}\newcommand{\elimina}[1]{}\newtheorem{lemma}{Lemma}\newtheorem{ese}{Esercizio}\newtheorem{problem}{Problem}\newtheorem{theorem}{Theorem}\newtheorem{corollary}{Corollary}\newtheorem{definition}{Definition}\newtheorem{conjecture}{Conjecture}\newtheorem{observation}{Observation}\newtheorem{proposition}{Propositon}\newcommand{\w}{{\bf w}}\newcommand{\inat}{{\bf I}\!{\bf I}\mskip -7mu{\bf N}}\newcommand{\x}{{\bf a}}\newcommand{\y}{{\bf b}}\newcommand{\F}{{\cal F}}\newcommand{\R}{{\sf R\hspace*{-0.9ex}\rule{0.15ex}%       {1.5ex}\hspace*{0.9ex}}}\newcommand{\N}{{\sf N\hspace*{-1.0ex}\rule{0.15ex}%       {1.3ex}\hspace*{1.0ex}}}\newcommand{\Q}{{\sf Q\hspace*{-1.1ex}\rule{0.15ex}%       {1.5ex}\hspace*{1.1ex}}}\newcommand{\C}{{\sf C\hspace*{-0.9ex}\rule{0.15ex}%       {1.3ex}\hspace*{0.9ex}}}\newcommand{\proof}[1]{{\noindent {\bf Proof.} {#1} \hfill\done}}\section{OR-Decision tables and Decision Trees}Given a set of conditions and the corresponding actions to be performed according to the outcome of these testing conditions, we can arrange them in a tabular form ${\cal T}$ called a {\em decision table}: each row corresponds to a particular outcome fot the conditions and is called {\em rule}, each column corresponds to a particular action to be performed. A given entry ${\cal T}[i,j]$ of the table is set to one if the action corresponding to column $j$ might be performed given the outcome of the testing condition as in row $i$; the entry is set to zero otherwise.  Different rules might have different probability to occur and testing conditions might be more o less expensive to test. The order in which conditions are tested might be influent or not. Decision tables are usually used to described the behaviour of a system whose state can be represented as a vector, the outcome of testing certain conditions. Given a particular state, the system evolves by performing a set of actions that depends on the given state of the system. The state is described by a particular rule, the action by the corresponding row of the table. There are different kind of decision tables, according to the system they have to describe: a table is said to be {\em limited} if the outcome of the testing conditions is binary and {\em extended} otherwise; a table in which there is a row for any rule ({\em i.e.}, having $k^L$ rows for $L$ conditions assuming $k$ different possible valuse) is said to be an {\em expanded} decision table, if a row might represent a set of rules that decide a common action the table is said to be {\em compressed}; a decision table is said to  be an {\bf AND}-decision table if {\bf all} the actions in a row must be executed when the corresponding rule occurs;  a decision table is said to  be an {\bf OR}-decision table if {\bf any} of the actions in a row might be executed when the corresponding rule occurs. The great advantage of decision tables is that they are easy to read and understand,  and  that they can be automatically converted into a program by means of a decsion tree: a tree that specify the order in which  to test the conditions (depending on the outcome of  previous tests) to decide which actions to execute. Changing the order in which conditions are tested might lead to more or less tests (and hence, higher or lower cost) to be performed to decide which actions to execute. The optimal decision tree is the one that requires the minimum cost when deciding which actions execute. Schumacher in \cite{} proposed a Dynamic Programming technique which guaranties to find the optimal decision tree given a limited decision table in which each row contains only one non-zero value. This strategy can be used for any limited {\bf AND}-decision table, defining a new {\em composed action} for any different row vector and substituting single action colums with composed actions. In this way  we obtain a limited {\bf AND}-decision table in which each row has only one non-zero value. Lew in \cite{} gives a Dynamic Programming approach for the case of extended and/or compressed {\bf AND}-decision tables.  \subsection{Preliminaries and notation}In this paper we will take into consideration limited {\bf OR}-decision tables and we will assume that the conditions are independent, {\em i.e.}, there are no constraint on the order in which conditions are tested.  We will give the full algorithm and correctness proof for the case of expanded tables (any compressed tabel can be easily expanded).We will use the following notation:\begin{itemize}\item $C= \{ c_1, \ldots, c_L \} $ boolean conditions; the cost of testing condition $c_i$ is given by $w_i >0$; \item $R = \{ r_1, \ldots, r_{2^L} \}$ rules; each rule is a boolean vector of $L$ elements, element $i$ corresponding to the outcome of condition $c_i$; the probability that rule $r$ occurres is given by $p_r \geq 0$;\item $A =\{ a_1, \ldots, a_M \}$ actions;  rule $r_i$ is associated to a non empty subset $A^{(i)} \subseteq A$ to be executed  when the outcome of conditions $c_j$s identifies rule $r_i$.\end{itemize}We wish to determine an efficient way to test conditions $c_1, \ldots, c_L$ in order to decide {\em as soon as possible} which action to be executed according to the values of conditions $c_i$s. In particular, we wish to determine in which order conditions $c_i$s have to be checked, so that the minimum number of tests are performed to determine which actions has to be taken (bla bla). %To this aim we build what is called a decision tree: a binary tree that ....We give an algorithm that builds an optimal decision tree given an  {\bf OR}-decision table; the same algorithm can be applied to the case in which rows contain only one non-zero entry (as a special case of  as  {\bf OR}-decision tables) and, hence, also to  {\bf AND}-decision tables in which single actions have been replaced by composed actions.\bigskipBefore formally defining what a decision tree is, we introduce the concept of $k-cube$: \begin{definition}[k-cube]A {\em k-cube} $K \subseteq R$ is a subset of the set of rules in which the value of $L-k$ conditions is fixed. It can be represented asa $L$-vector of $k$ dashes ($-$) and $L-k$ values 0's and 1's . We will denote the set of positions containing dashes as $D_K$, and $[1..L]\setminus D_K$ as $V_K$. The set of actions $A_K = \cap_{r \in K} A^{(r)}$  is associated to cube $K$ (might be an empty set).  \end{definition}\begin{observation}If $A_K \not= \emptyset$ any action in $A_K$ might be executed in association to any rule in $K$ ({\em i.e.}, there is no need to distinguish between rules in $K$ to decide the action to be performed), hence there is no need to test conditions $c_i$s corresponding to dashes, to decide which actions might be executed. \end{observation}  \bigskip\begin{definition}[Decision Tree]Given $k$-cube $K$, with dashes in positions in $D_K \subseteq [1..L]$ and values $0$s and $1$s in $V_k\subseteq [1..L]$, $k \leq L$, a {\em Decision Tree} for $K$ is a binary tree $T$ with the following properties: \begin{enumerate}\item Each leaf $\ell$ corresponds to a set of rules $K_{\ell} \subseteq K$ and $K_{\ell}$ is a $k'$-cube, with $0\leq k'\leq k$.  Given the set of leaves $\ell_1, \ldots, \ell_n$ of the tree, $1\leq n \leq 2^L$, then $\{ K_{\ell_1} , \ldots,  K_{\ell_n}\}$  is a partition of $K$ into cubes.  \item Each internal node $v_i$ corresponds to a testing condition $c_{j_i} \in C$ for some $j_i \in V_K$. \item Arcs are labeled with one value in $\{ 0, 1 \}$.\item Each root-leaf path $v_1,\ldots, v_m$ leading to leaf $\ell$, corresponding to cube $K_{\ell}$ of dimension $k'\leq k$, is identifed by an ordered sequence of pairs $$ (c_{j_1},o_1),(c_{j_2},o_2), \ldots, (c_{j_m},o_m)$$where, $1\leq m \leq L-k'$,  $c_{j_i} \in C$ be the condition associated to node $v_i$ and $o_i$ is the label of the outgoing arc on the path.  Moreover, the following properties hold:\begin{enumerate}\item[I.] $c_{j_i} \not= c_{j_t}$, for every $i \not= t$, $i, t \in \{ 1, \ldots, m\}$;  {\em i.e.}, the testing conditions associated to nodes on the path are distinct. \item[II.] The set of rules associated to leaf $\ell$ is the cube having: (a) values $0$'s and $1$'s setted as the labels on the root-leaf path for the corresponding $c_{j_i}$s, (b) conditions in positions $V_K$ setted as in the cube $K$ and (c) dashes in the remaining positions ({\em i.e.}, those in $D_K \setminus \{ j_1, \ldots, j_m\}$) .\end{enumerate}\item Each leaf $\ell$ is associated to the set of actions $A_{K_{\ell}}$,  those associated to cube $K_{\ell}$.\item The decision tree is associated to the intersection of the actions associated to the leaves of the tree; {\em i.e.},  $A_T = \cap_{\ell \in {\cal L}} A_{K_{\ell}}$, where ${\cal L}$ is the set of leaves of the tree. \end{enumerate}\end{definition}\begin{definition}[Path cost]Given a Decision Tree $T$ for a cube $K$, let ${\cal P_{\ell}} = v_1,\ldots, v_m$ be a root-leaf path leading to leaf $\ell$, $1\leq m\leq |D_K|$, and let $c_{j_i}$ be the condition associated to node $v_i$, $i=1,\ldots, m$. The cost of the path leading to leaf $\ell$ is defined as the sum of the costs of the conditions corresponding to nodes on ${\cal P_{\ell}}$: $$ path(\ell) = \sum_{i =1}^m w_{j_i}.$$\end{definition}\begin{definition}[Occurence probability of a Decision Tree]The occurence probability $P_T$ of a decision tree $T$  for a $k$-cube $K$ is the  probability $P_K$ of any rule in $K$ to occur, {\em i.e.} it is the sum of the occurrence probabilities of the rules in the cube: $$ P_T =  P_K = \sum_{r \in K} p_r .$$In the particular case in which the tree is a leaf $\ell$, we will denote the occurence probability of the leaf as $P_{\ell}$. \end{definition}Observe that all distinct decisional trees on the same cube occur with the same probability, as the union of the rules associated to the leaves is the same for all trees and it is equal to $K$. \medskipThe average cost of a decision tree is a measure of the cost of testing the conditions that we need to check to decide which actions to take when rules occur, weighted by the probability that rules occur. We wish this cost to be as small as possible, especially for rules that occur frequently.  It is formally defined in the following way: \begin{definition}[Average Cost of a Decision Tree]Let $R$ be a $L$-cube such that $P_R=1$, let $T$ be a decision tree for $R$ and  ${\cal L}$ be the set  leaves in $T$. The {\em Average cost} of decision tree $T$ is given by $$ \overline{cost}(T) = \sum_{\ell \in {\cal L}} P_{\ell} \, path(\ell),$${\em i.e.}, the sum, over all the leaves of the tree, of the costs of the root-leaf path multiplied by the probability on any rule in the corresponding leaf to occur. An {\em Optimal Average Cost Decision Tree} for $L$-cube $R$ is a decision tree for the cube with minimum average cost (might not be unique). \end{definition}The algorithm that we will describe in the next section finds a decision tree of minimum average cost for the $L$-cube that describes a given {\bf OR}-Decision table. \begin{definition}[Tree-compatible partition]Given a $k$-cube $K$ with dashes in positions in $D_K \subseteq [ 1..L] $ and a partition of $K$ into two sets $K_0$ and $K_1$, the partition is said to be {\em Tree-compatible} if $K_0$ and $K_1$ are both  $(k-1)$-cubes with the $k-1$ dashes in the same positions $D'_{K} \subset D_K$.  Index $i \in D_K \setminus D'_{K}$ is said to {\em distinguish} between $K_0$ and $K_1$. \end{definition}Observe that there are $k$ distinct tree-compatible partition for any $k$-cube $K$, one for each different index in $D_K$. Moreover, the two subcubes have dashes in the same positions give by set $D_K \setminus \{ i\}$, where index $i$ distinguishes between the two subcubes. All rules of one subcube have condition in position $i$ setted to zero, while those in the other subcube have that condition setted to one. \bigskipObserve also that the cubes corresponding to the two subtrees of a decision tree, must form a tree-compatible partition. As an example, consider, cube $K = \{ 00, 01, 10, 11\}$, the non tree-compatible partion $K'= \{ 00 \}, K'' =\{ 01, 10, 11\}$ and assume there is no common action for the rules in the two sets. Hence, the decision tree must have at least one internal node. Assume we palce an internal node corresponding to the first testing condition. To satisfy property 4.b  of decision trees, rules of $K'$ are to be placed in the subtree reached by following the outgoing arc labeled with zero, while rules of $K"$ should be placed in the subtree reached  by following the outgoing arc labeled with one.  But this is not possible as rule  $01 \in K''$ would not satisfy property 4.b.Analogously, assume we palce an internal node corresponding to the second testing condition, then rules of $K'$ belong to the subtrees reached by following the outgoing arc labeled with zero to satisfy property 4.b, and hence rules in $K''$ are to be placed in the subtree reached  by following the outgoing arc labeled with one. Again, this is impossible, as rule  $10 \in K''$ does not satisfy property 4.b.\bigskip\begin{proposition}\label{prop:unione}Given a $k$-cube $K$ and any tree-compatible partition $\{ K_0,K_1\}$ for $K$, distinguished by some index $i\in D_K$, we have$$ P_K = P_{K_0} + P_{K_1} \,\,\,\,\,  \mbox{ and } \,\,\,\,\,  A_K = A_{K_0} \cap  A_{K_1} .$$\end{proposition}\proof{ The proof follows directly form the fact that, for any tree-compatible partition, the union of the subcubes is always the entire cube.}\medskipWe extend the notion of average cost to decision trees on cubes such that  the probabilities of the rules in the cubes do not sum up to one. In this case, the quantity will be  called simply $cost$ and is defined in the following way: \begin{definition}[Cost of a Decision Tree and Optimal Decision Tree]\label{def:cost}Given $k$-cube $K$  and a decision tree $T$ for $K$, the {\em cost}  of $T$ is defined in the following way:\begin{equation}\label{eq:cost} cost(T) =   \sum_{\ell \in {\cal L} } P_{\ell} \, path(\ell) .\end{equation}An {\em Optimal Decision Tree} for $k$-cube $K$ is a decision tree for the cube with minimum cost (might not be unique). If $P_K=0$  (meaning that, for any rule $r \in K$, $p_r = 0$), an Optimal Decision Tree is a leaf of cost equal to zero.\end{definition}Observe that, if $P_K =0$ for cube $K$, any decision tree for $K$ has cost equal to zero and no rule of the cube will ever occur. Hence, we can place all rules in a single leaf (even if the cube is associated to an empty set of actions) to have the smallest possible tree representation of the cube. \begin{proposition}Given cube $K$ such that $P_K=1$, the Cost of a Decision Tree for $K$ is also the Average Cost of $T$. In particular, an optimal decision tree for $L$-cube $R$, is also an optimal average cost decision tree for $R$.\end{proposition}The Dynamic Programming algorithm that we will present in the next section, computes an optimal decision tree by looking for the tree having maximum gain, instead of minumum cost. The gain of a tree counts the cost of the testing conditions that can be avoided to distinguish between rules, taking into consideration the occurence probability of rules: we wish to test less, and less expensive, conditions for rules with higher occurence probability, while we accept to test more, and more expensive, conditions for rules occurring less frequently. The gain of a decion tree $T$ is formally defined as the difference between the maximum cost achievable by any decision tree and the cost of  tree $T$. First, we will characterize decison trees, for a given cube, having maximum cost  as complete binary trees in which exaclty one rule is associated to each leaf. Using this characterization we will, then, formally define the concept of gain. Finally, we will give an alternative mathematical formulation that will be more useful to proof the correcteness of the algorithm. \begin{lemma}Given $k$-cube $K$, $0\leq k \leq L$, with dashes in positions $D_K$, any decision tree ${\cal T}$ that  is a compelte binary tree of height $|D_K|$, having exactly one rule associated to each leaf, has cost given by $$ cost({\cal T}) = P_K \sum_{i \in D_K}  w_i,$$and this cost is maximum over all possible decision trees for $K$. \end{lemma}\proof{Let ${\cal L}$ be the set of leaves  of ${\cal T}$. First, we will compute the cost of tree  ${\cal T}$, than we will show that this cost is the maximum achievable by any decision tree for $K$. Observe that, in the complete decision tree, all root-leaf paths are exaclty $|D_K|$ nodes long and there is exactly  one node corresponding to each testing condition. Hence, for any leaf $\ell$, we have $path(\ell) = \sum_{i\in D_K} w_i$ and  the cost of tree ${\cal T}$ is given by:\begin{eqnarray*}cost({\cal T}) &=& \sum_{\ell \in {\cal L}} P_{\ell} \, path(\ell) \\&=& \sum_{\ell \in {\cal L}} \left[ P_{\ell} \sum_{i\in D_K} w_i \right] \\&=& \left[ \sum_{i\in D_K} w_i \right] \cdot \left[ \sum_{\ell \in {\cal L}} P_{\ell} \right] = P_K \sum_{i \in D_K}  w_i.\end{eqnarray*}Consider, now, any other decision tree $T$ for $K$, we prove that $cost({\cal T}) - cost(T) \geq 0$. First observe that, as there is only one rule $r$ associated to each leaf $\ell$ in ${\cal T}$,  then $P_{\ell} = p_r$. The cost of ${\cal T}$ can, thus, be expressed as:$$cost({\cal T}) = \sum_{\ell \in {\cal L}} P_{\ell} \, path(\ell) = \sum_{r \in K} \left[ p_r \cdot \sum_{i\in D_K} w_i \right].$$Given any other decision tree $T$ for $K$,observe that the cube corresponding to a leaf in $T$ must have dashes in a subset of the positions in $D_K$. Hence, given a rule $r$ that belogns to a given leaf $\ell$, let $V_r \subseteq D_K$ be the positions in which in cube $K_{\ell}$ has values $0$'s and $1$'s while there are dashes in cube $K$ ({\em i.e.}, the positions corresponding to variables on the root-leaf path). We can express the cost of $T$ as the sum of the costs {\em brought} by single rules in $K$:   $$ cost(T) = \sum_{\ell \in {\cal L}} \left[ \sum_{r \in {\ell}} p_r \cdot path(\ell) \right]=\sum_{\ell \in {\cal L}} \sum_{r \in \ell}\left[  p_r  \cdot \sum_{i\in V_{r}} w_i \right]= \sum_{r \in K} \left[ p_r \cdot \sum_{i\in V_{r}} w_i \right].$$Hence, \begin{eqnarray} cost({\cal T}) - cost(T) &=& \sum_{r \in K} \left[ p_r \cdot \sum_{i\in D_K} w_i \right] - \sum_{r \in K} \left[ p_r \cdot \sum_{i\in V_{r}} w_i \right] \nonumber \\&=& \sum_{r \in K} \left[ p_r \cdot \sum_{i\in D_K\setminus V_{r}} w_i \right]  \geq 0, \label{eq:gain2}\end{eqnarray}as $p_r, |D_K\setminus V_{r}|, w_i \geq 0$, for all $r$ and for all $i$.}\begin{definition}[Gain of a Decision Tree]\label{def:gain}Given a decision tree $T$ for $k$-cube $K$ and a decision tree ${\cal T}$ with highest cost for $K$, the {\em gain} of $T$ is defined as:   $$ gain(T) =  cost({\cal T}) - cost(T).$$\end{definition}Obviously, minimising the cost of a tree is equivalent to maximising the gain ot the tree and {\em vice-versa}. \begin{proposition}A Decision Tree for a cube is optimal if and only if it has maximum gain. \end{proposition}\medskipThe following lemma gives an alternative mathematical formulation of the gain, more sutable for the following proofs. \begin{lemma}\label{lemma:gain}Given $k$-cube $K$  and a decision tree $T$ for $K$,  the {\em gain} of $T$ can be stated in the following way:\begin{equation}\label{eq:gain} gain(T) = \sum_{\ell \in {\cal L} }  \left[ P_{\ell}  \sum_{i \in D_{\ell}} w_i \right], \end{equation}where $D_{\ell}$ is the set of positions of dashes in cube $K_{\ell}$.\end{lemma}\proof{ Observe that $D_K\setminus V_{r} = D_{\ell}$, where $\ell$ is the leaf $r$ belongs to; {\em i.e.},  $D_K\setminus V_{r}$ is the set of positions of dashes in $K_{\ell}$. Starting again from equation~(\ref{eq:gain2})we have: \begin{eqnarray*}gain(T) &=& cost({\cal T}) - cost(T) \\&=& \sum_{r \in K} \left[ p_r \cdot \sum_{i\in D_K\setminus V_{r}} w_i \right] \\&=& \sum_{\ell \in {\cal L}} \sum_{r \in K_{\ell}} \left[ p_r \cdot \sum_{i \in D_{\ell}} w_i \right] \\&=& \sum_{\ell \in {\cal L}}  \left[ P_{\ell} \sum_{i \in D_{\ell}} w_i \right] \\\end{eqnarray*}}Observe that, if a tree is a leaf, the gain of a leaf is still well defined, as the summation in equation~\ref{eq:gain} has exactly one term. Moreover, if a leaf contains exaclty one rule, then the $0$-cube corresponding to the rule has no dashes, hence the summation over indices in $D_{\ell}$ is empty and the gain of the leaf is zero. \medskipMoreover, if a leaf has probabilitiy zero to occur, the gain is zero. This makes sense, as there is no possible gain coming from rules that will never occur. \bigskipBefore presenting the algorithm, we investigate a general property of Decision Trees that will be useful to prove the correctness of the algorithm. \begin{lemma}\label{lemma:leaves}Given $k$-cube $K$ ($1 \leq k \leq L$), let $A_K = \cap_{r\in K} A^{(r)}$ be the set of common action executable for rules in $K$. If $A_K$ is not empty, then the optimal decision tree is composed of only one node (a leaf).\end{lemma}%\begin{IEEEproof}\proof{Assume, by contradiction, that there exist an optimal decision tree $T$ with more than one node and optimal $gain(T) = OPT$. Take two sibling leaves $\ell_0$ and  $\ell_1$ such that $P_{\ell_0} >0$ or $P_{\ell_1}>0$. If such a pair of leaves does not exist, then $P_K=0$ and, by defintion, the optimal decision tree is a leaf, contraddicting the hypotesis that $T$ has more than one node. Hence, assume $\ell_0$ and  $\ell_1$ are sibling leaves such that: (1) $P_{\ell_0} >0$ or $P_{\ell_1}>0$; (2) dashes of their correspongin cubes are in positions in set $D$ (being siblings, the set of positions is the same) (3)  their parent is node $v$, corresponding to testing condition $c_i$, for some $1\leq i \leq L$ and $i \not\in D$. As $A_K \not= \emptyset$, there must be a non empty set $A_v= A_{\ell_0} \cap A_{\ell_1}$ of common actions between the rules in $\ell_0$ and $\ell_1$. Build a new decision tree $T'$ for $K$ by replacing  node $v$ in $T$ with a new leaf $\ell$ that contains all rules in leaves $\ell_0$ and $\ell_1$ ({\em i.e.}, $K_{\ell} = K_{\ell_0} \cup K_{\ell_1}$), and associate actions in $A_v$ to the new leaf ({\em i.e.}, $A_{\ell} = A_v \not= \emptyset$, no test is needed to distinguish between rules in $K_{\ell}$). The new tree $T'$ has gain \begin{eqnarray*}gain(T')&= & gain(T)  - \left[ gain(\ell_0) + gain(\ell_1) \right] + gain(\ell) \\&=& OPT - \left[ P_{{\ell_0}} \sum_{j \in D} w_j  +  P_{\ell_1} \sum_{j \in D} w_j \right] + P_{\ell}  \sum_{j \in D\cup \{i\}} w_j \\&=& OPT - \left[ \left( P_{{\ell_0}} + P_{{\ell_1}}  \right) \sum_{j \in D} w_j \right] +  \left(P_{{\ell_0}} + P_{{\ell_1}} \right) \sum_{j \in D} w_j  +  P_{{\ell}} w_i  \\&=&  OPT +   P_{{\ell}} w_i > OPT,\end{eqnarray*}as $P_{{\ell}}= P_{\ell_0} + P_{\ell_1} >0$ and $ w_i>0$. This is a contraddiction because $T$ was supposed to have maximum gain. %\end{IEEEproof}}\subsection{Dynamic Programming Algorithm}An optimal decision tree can be computed using a generalization of the Dynamic Programming strategy introduced by Schumacher and Sevcik in \cite{}, with a bottom-up approach: staring from $0$-cubes and for increasing dimension of cubes, the algorithm computes the gain of all possible trees for all cubes and keeps track only of  the ones having maximum gain. Here we give a formal description of the algorithm and a formal proof of its correctens. \elimina{The main differences among the approach to {\bf AND}-decision tables and to {\bf OR}-decision tables is in the decision of which actions to associate to a set of rules. In the former case, two different rules are associated to the same set of actions if and only if both rules are associated to identical rows. In the latter, the algorithm associates the intersection of the sets of actions associated to the rules and these choiche is proven to be the one leading to the optimal solution. }\medskipThe algorithm works in the following way:\medskip\noindent {\bf ALGORITHM:} {\sc MGDT - Maximum Gain Decision Tree for OR-Decision Tables}\begin{itemize}\item Step $0$: all $0$-cubes ({\em i.e.}, all rules) are associated with the set of actions associated to the rule in the cube; the occurrence probability of the cube is set equal to the occurence probability of the rule in the cube and the gain is set to zero. \item Step $k$ ($1 \leq k \leq L$): for each $k$-cube $K$ with dashes in positions in $D_K$ find  a tree-compatible partition that leads to a decison tree with maximum gain in the following way:\begin{enumerate}\item[a.] From the previous step we have: (1) the occurrence probabilities $P_0$ and $P_1$ of the two subcubes in any tree-compatible partition for $K$ and (2) the set of actions $A_0$ and $A_1$ associated to the same subcubes. Compute the occurence probabiliy $P_K = P_0 + P_1$ of $K$ and the set of actions  $A_K = A_0 \cap A_1$ associated to $K$. If $P_K=0$, then (1) set gain for the tree to zero, (2) keep $P_K$ for the construction of an optimal decision tree in step $L+1$ and (3) procede to step $k+1$.Otherwise, keep $P_K$ and $A_K$ to be used in step $k+1$ and procede to substep $b$.\item[b.]  For each $i \in D_K$, compute the gain of the decision trees $T_i$, corresponding to the tree-compatible partition $\{ K_{i,0}, K_{i,1}\}$. Using information computed at step $k-1$ about maximum gain of decision subtrees $T_{i,0}$ and $T_{i,1}$ for $(k-1)$-cubes $K_{i,0}$ and $K_{i,1}$, respectively, in the following way:$$ gain(T_i) = \rho_i \cdot P_K \cdot  w_i  + gain(T_{i,0}) + gain(T_{i,1}), $$where $$\rho_i = \left\{ \begin{array}{ll}                1 & A_K \not= \emptyset \\                0 & \mbox{ otherwise.}        \end{array} \right.       $$ Keep one $i_K \in D_K$ leading $T_{i_K}$ with maximum gain ({\em i.e.}, $i_K \in \arg\max_{i\in D_K} gain(T_i)$), and $A_K$ for the construction of an optimal decision tree in step $L+1$.%\item Keep the occurence probability of $T_{i^*}$ given by $P_{i^*} = P_{i^*,K_0}+ P_{i^*,K_1}$;%\item Keep the set of actions associated to $T_{i^*}$ given by $A_{i^*} = A_{i^*,T_0} \cap A_{i^*,T_1}$. \end{enumerate}\item Step $(L+1)$: To actually build the optimal decisional tree, work backwards in order to decide which tree-compatible partiton lead the optimal gain by means of the $i_K$'s. Given cube $K$, $P_K$ or the pair index $i_K$ and set of actions $A_K$ associated to $K$, procede in the following way:\begin{itemize}\item If $P_K = 0$, create a leaf associated to rules in $K$ and associate an empty set of actions.\item Otherwise: \begin{itemize}\item If $A_K = \emptyset$, build a tree having root in a new node $v$ associated to testing condition $c_{i_K}$ and subtrees build recursively as optimal decision tree on tree-compatible partition decided by index $i_K$;\item If $A_K \not= \emptyset$, create a leaf associated to rules in $K$ and set of actions $A_K$.\end{itemize}\end{itemize}\end{itemize}\medskipObserve that at step $k$ ($1 \leq k \leq L$) there are ${L \choose k} 2^{L-k}$ distinct $k$-cubes to analyze: there are ${L \choose k}$ possible ways to place $k$ dashes among $L$ positions and, for each such choice, there are $2^{L-k}$ different ways to fix the other $L - k$ conditions to values $0$ and $1$.  The total number of cubes that are analysed by the algorithm is $3^L$, all possible words of length $L$ on the three letter alphabet $\{ 0, 1, - \}$. \subsubsection{Example}\subsubsection{Correctness}We now prove that the gains computed by the algorithm are actually the gains defined in Definition~\ref{def:gain} (expressed as in Lemma~\ref{lemma:gain}) and, then, that the computed decision tree is one having maximum gain. \begin{observation}If a decisional tree computed by the algorithm {\sc MGDT} has an internal node, then there is no action that can be exectued for all the rules associated to the leaves of the tree. {\tt serve ancora? }\end{observation}\begin{proposition}Steps $k.a$, $1\leq k \leq L$, of the algorithm computes $P_K$ and $A_K$ correctly, by means of Proposition~\ref{prop:unione}. \end{proposition}\begin{lemma}\label{lemma:leaf_cost} Let  $K$ be a $k$-cube ($1 \leq k \leq L$) with dashes at indices $D_K $ and  $A_K$ the set of associated common actions, then the tree build by algorithm  {\sc MGDT} has gain as given in Lemma~\ref{lemma:gain}.\end{lemma}%\begin{IEEEproof}\proof{Proof is by induction. {\em Base case}: at step $0$,  the algorithm builds leaves whose correspondign cube is one rule with associated (non empty set of) actions and gain zero. This is coherent with the definition of gain, as, for any $0$-cubes $K$, $D_K = \emptyset$, hence the summation in  equation~(\ref{eq:gain}) is empty, and the gain is zero. The leaf is created also if the rule has probability zero to occur. \medskip {\em Inductive hypotesis} ($k-1 \geq 0$): given $(k-1)$-cube $K'$ with dashes in position $D_{K'}$ such that $A_{K'}$ is the set of common action associated to rules in the cube, then the tree computed by algorithm {\sc MGDT} has gain as given in Lemma~\ref{lemma:gain}. Moreover, if $A_{K'} \not= \emptyset$ or $P_{K'}=0$, the tree built by the algorithm is a leaf.  \medskip{\em Inductive step}: Consider $k$-cube $K$ and the tree  $T$ built  at step $k$ of the algorithm, by means of trees $T_{i,0}$ and $T_{i,1}$, corresponding to $(k-1)$-cubes $K_{i,0}$ and $K_{i,1}$ with dashes at indices $D'_K \subset D_K$ and such that $i \not\in D'_K$.  The occurence probability of $K$ is (correctly) computed as $P_K = P_{K_{i,0}} + P_{K_{i,1}}$ and the set of corresponding action (correctly) set to $A_K = A_{K_{i,0}} \cap A_{K_{i,1}}$.  We distinguish three cases: \begin{itemize}\item $P_K =0$, then the algorithm (step $k.a$) builds a leaf with gain zero, gain choerent with Equation~\ref{eq:gain} (as all the rules must have probility zero to occur as well).\item $P_K \not= 0$ and $A_K \not= \emptyset$, hence $\rho_i = 1$ and it must be $A_{K_{i,0}}, A_{K_{i,1}} \not= \emptyset$. By   inductive hypotesis, trees $T_{i,0}$ and $T_{i,1}$ built by the algorithm are leaves and have gain given by Equation~\ref{eq:gain}. Algorithm {\sc MGDT}  makes $T$ a leaf $\ell$ and computes gain in the following way:\begin{eqnarray*}gain(\ell) &=& \rho_i P_K w_i + gain(T_{i,0}) + gain(T_{i,1})   \\& & \\& & \mbox{as } \rho_i = 1  \mbox{ and using the inductive hypotesis, we have} \\& & \\& =& P_K  w_i +  P_{K_{i,0}} \sum_{j \in D'_K } w_{j}  + P_{K_{i,1}} \sum_{j \in D'_K } w_{j}  \\ &=&  P_K  w_i  + (P_{K_{i,0}}  +P_{K_{i,1}} ) \sum_{j \in D'_K } w_{j} \\&=& P_K \left( w_i + \sum_{j \in D'_K } w_{j} \right) \\&=& P_K \sum_{j \in D_K} w_j\end{eqnarray*}\item $P_K \not= 0$ and $A_K = \emptyset$, hence $\rho_i = 0$, and, by inductive hypotesis, trees $T_{i,0}$ and $T_{i,1}$ have gain given by Equation~\ref{eq:gain}. Algorithm {\sc MGDT}  builds $T$ by adding an internal node $v$ associated to testing condition $c_i$ and by making  $T_{i,0}$ and $T_{i,1}$ the left and right, respectively, subtrees. Given ${\cal L}_0$ and  ${\cal L}_1$ the set of leaves of $T_{i,0}$ and $T_{i,1}$, respectively,  the set of leaves of $T$ is ${\cal L} = {\cal L}_0 \cup {\cal L}_1$ and the gain of $T$ is computed in the following way:\begin{eqnarray*}gain(T) &=& \rho_i P_K w_i + gain(T_{i,0}) + gain(T_{i,1})   \\& & \\& & \mbox{as } \rho_i = 0  \mbox{ and using the inductive hypotesis, we have} \\& & \\&=& \sum_{\ell \in {\cal L}_0} \left[ P_{\ell} \sum_{i \in D_{\ell}} w_i \right] + \sum_{\ell \in {\cal L}_1}  \left[ P_{\ell} \sum_{i \in D_{\ell}} w_i \right] \\&=& \sum_{\ell \in {\cal L}}   \left[ P_{\ell} \sum_{i \in D_{\ell}} w_i \right]\end{eqnarray*}\end{itemize}}%\end{IEEEproof}\begin{theorem}Given an expanded {\bf OR}-Decision Table described by means of set of conditions $C$, set of rules $R$ and  set of actions $A$, algorithm {\sc MGDT} computes an optimal decision tree.\end{theorem}%\begin{IEEEproof}\proof{ We prove that the algorithm computes an optimal decision tree by showing that it computes a tree with maximum gain. The proof is by induction. {\em Base case} $k=0$: $0$- cubes have only one possible decision tree with gain zero and these are determined correcty by step $0$ of the algorithm. {\em Inductive hypotesis}:  the algorithm computes maximum gain trees for cubes up to dimension $k-1$. {\em Inductive step}:  Given a $k$-cube $K$ with dashes in positions $D_K$, there are two possibilities:\begin{itemize}\item The tree $T$ computed by the algorithm is a leaf,  then either $P_K=0$, or the set of action associated to the cube is not empty. By definition or by Lemma~\ref{lemma:leaves}, respectively, this implies that the optimal tree for the cube is a leaf. The algorithm computes correct gains for leaves, hence,  the tree is optimal. \item The tree $T$ computed by the algorithm is not a leaf. Then,  $A_T = \emptyset$ and there exists $i^* \in D_K$ such that $T_{0}$ and $T_{1}$ are the subtrees of $T$ (corresponding to tree-compatible partition $\{ K_0,K_1\}$ distinguished by index $i^*$) and the root of $T$ corresponds to testing condition $c_{i^*}$. By construction, partition $\{ K_0,K_1\}$ induces the highest gain  over all possible tree-compatible partition of $K$. By inductive hypotesis, $T_{0}$ and $T_{1}$ have maximum gain with respect to all possible trees over $K_0$ and $K_1$, respectively. Assuming there is a tree with higher gain, means assuming the existence of a tree-compatible partition of $K$ leading to a gain higher than maximum.  This is a contradiction. \end{itemize}}%\end{IEEEproof}\subsubsection{Computational time}\begin{theorem}Given an expandend {\bf OR}-Decision Table described by means of set of conditions $C$, set of rules $R$ and  set of actions $A$, the algorithm computes an optimal decision tree in  $O(|A| \cdot |C| \cdot |R|^2)$ worst case computational time. \end{theorem}%\begin{IEEEproof}\proof{The algorithm considers $3^L$ cubes.  For each $k$-cube it computes the gain of $k \leq L$ trees, one for each tree-compatible partition. For each tree-compatible partion, the algorithm computes the intersection of the actions associated to the subtrees induced by the partition; this task that can be accomplished in $O(|A|)$ wost case computational time. Hence, the computational time of the dynamic programming strategy is upper bounded by: \begin{eqnarray*} 3^L \cdot L \cdot |A| &\leq& (2^2)^L \cdot L \cdot |A| \\&=& (2^L)^2 \cdot L \cdot |A| \in O(|A| \cdot |C| \cdot |R|^2). \end{eqnarray*}}%\end{IEEEproof}\subsubsection{About compressed decision tables}One might think that the algorithm might be used also in the case of compressed {\bf OR}-Decision tables, by just making a leaf associated to all rules in the cube that corresponds to a compressed rule ({\tt si chiamano cosi'?}). In this section we give a very simple example showing that, this approach, does not lead to the optimal decision tree. Hence, to derive a decision tree starting from a compressed table, we have to expand the table or use a different approach.     \begin{figure}[thb]  \centering  \epsfig{figure=contro_es.eps, width=8truecm}  \caption{$|C| = |A| = 2$, $w_i=1$ for all conditions, $p_i=1/4$ for all rules,  action $a_1$ associated to rule $01$, actions $\{ a_1,a_2\}$ to rules $1-$, action $a_2$ to rule $00$. The tree build by taking $1-$ as a ``block'' has gain $1/2$, if we split the block we get gain $1$. }  \label{fig:contro_esempio}\end{figure}\section{Application to block-based Connected Components Labeling?}Confronto con albero dell'altro paper?{\tt REMINDER} Quando si usa per le AND-decision tables trasformate in entrate singole: il numero di azioni diverse e' al massimo il numero di regole (non ha senso il contrario), ma per fare l'intersezione ci metto O(1), quindi la complessita' (senza considerare la trasformazione della tabella) e' $O(|C| \cdot |R|^2)$. \end{document}